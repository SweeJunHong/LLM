{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"you_api_key\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"you_api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.schema import Document\n",
    "from langchain.llms import OpenAI\n",
    "import cohere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load & Process Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2269, which is longer than the specified 512\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "loader = TextLoader(\"learning_material.txt\", encoding=\"utf-8\")  # Replace with sample document\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into manageable chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JunHong\\AppData\\Local\\Temp\\ipykernel_5028\\2393451350.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\JunHong\\OneDrive\\Desktop\\SIT\\Year 2 Tri 2\\LLM\\Project_draft\\llm_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Use Hugging Face Sentence Transformers (free, local) using openaiembeddings will get error\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Store embeddings in FAISS\n",
    "vector_db = FAISS.from_documents(docs, embedding_model)\n",
    "vector_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Hybrid Retrieval (BM25 + FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviations\n",
      "AES Advanced Encryption Standard\n",
      "AWGN additive white Gaussian noise\n",
      "BC broadcast channel\n",
      "BCC broadcast channel with confidential messages\n",
      "BEC binary erasure channel\n",
      "BSC binary symmetric channel\n",
      "CA certification authority\n",
      "DES Data Encryption Standard\n",
      "DMC discrete memoryless channel\n",
      "DMS  \n",
      "---\n",
      "Physical-Layer Security\n",
      "From Information Theory to Security Engineering\n",
      "This complete guide to physical-layer security presents the theoretical foundations, practical implementation, challenges, and benefits of a groundbreaking new model for secure\n",
      "communication. Using a bottom-up approach from the  \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Load FAISS index\n",
    "vector_db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "# need to set allow_dangerous_deserialization=True to load the index from disk\n",
    "# otherwise will get error on security\n",
    "\n",
    "# Initialize BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "# Hybrid retrieval function\n",
    "def hybrid_retrieve(query, top_k=5):\n",
    "    \"\"\"Retrieve documents using both BM25 and FAISS\"\"\"\n",
    "    bm25_results = bm25_retriever.get_relevant_documents(query)[:top_k]\n",
    "    faiss_results = vector_db.similarity_search(query, k=top_k)\n",
    "    \n",
    "    # Merge results, removing duplicates\n",
    "    seen = set()\n",
    "    combined_results = []\n",
    "    for doc in bm25_results + faiss_results:\n",
    "        if doc.page_content not in seen:\n",
    "            combined_results.append(doc)\n",
    "            seen.add(doc.page_content)\n",
    "    \n",
    "    return combined_results[:top_k]\n",
    "\n",
    "# Test retrieval\n",
    "query = \"Explain decoder\"\n",
    "retrieved_docs = hybrid_retrieve(query)\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content[:300], \"\\n---\")  # Show first 300 chars of each result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Re-Ranking with Cohere Rerank API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "def rerank_results(query, retrieved_docs):\n",
    "    \"\"\"Re-rank retrieved documents using Cohere Rerank API\"\"\"\n",
    "    rerank_inputs = [doc.page_content for doc in retrieved_docs]\n",
    "    response = co.rerank(model=\"rerank-english-v2.0\", query=query, documents=rerank_inputs, top_n=len(rerank_inputs))\n",
    "    \n",
    "    # Sort documents by relevance score\n",
    "    ranked_docs = sorted(zip(response.results, retrieved_docs), key=lambda x: x[0].relevance_score, reverse=True)\n",
    "    \n",
    "    return [doc[1] for doc in ranked_docs]\n",
    "\n",
    "# Apply re-ranking\n",
    "reranked_docs = rerank_results(query, retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response Generation with GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A decoder is a device or algorithm that converts encoded data back into its original format. In the context of communication systems, particularly those involving encoding for secure transmission, a decoder plays a crucial role in ensuring that the intended receiver can accurately recover the original message from the received codeword.\n",
      "\n",
      "In secure communication, messages are often encoded using specific coding schemes to protect against eavesdropping and to ensure reliable transmission over noisy channels. The encoding process transforms the original message (often referred to as the plaintext) into a codeword, which is then transmitted over the communication channel. The decoder, located at the receiving end, is responsible for interpreting the received codeword and reconstructing the original message.\n",
      "\n",
      "Key functions of a decoder include:\n",
      "\n",
      "1. **Error Correction**: Decoders often incorporate error-correcting techniques to handle noise and interference that may corrupt the transmitted codeword. This ensures that the intended message can be recovered even if some bits are altered during transmission.\n",
      "\n",
      "2. **Mapping Codewords to Messages**: The decoder uses the specific coding scheme employed during encoding to map the received codeword back to the original message. This process may involve looking up a codebook or applying mathematical transformations.\n",
      "\n",
      "3. **Maintaining Security**: In the context of physical-layer security, decoders may also be designed to ensure that even if an eavesdropper intercepts the codeword, they cannot easily extract the original message. This is achieved by exploiting the randomness and noise in the communication channel.\n",
      "\n",
      "Overall, the effectiveness of a decoder is critical for the reliability and security of communication systems, allowing legitimate receivers to accurately interpret the information while preventing unauthorized access by eavesdroppers.\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPT-4 model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "def generate_response(query, reranked_docs):\n",
    "    \"\"\"Generate response using GPT-4 with retrieved knowledge\"\"\"\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "    \n",
    "    prompt = f\"\"\"You are a personalized NLP learning assistant. Use the provided knowledge to answer the question accurately.\n",
    "\n",
    "    Knowledge Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.predict(prompt)\n",
    "    return response\n",
    "\n",
    "# Generate response\n",
    "final_response = generate_response(query, reranked_docs)\n",
    "print(final_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this session is for debugging / testing the openai api, \n",
    "# DO NOT NEED to run if the code above works\n",
    "# https://platform.openai.com/docs/api-reference/debugging-requests\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Say this is a test\",\n",
    "    }],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "print(response._request_id)\n",
    "print(response)\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personalization (User-Adaptive Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Interaction History: [{'query': 'Explain decoder', 'response': 'A decoder is a device or algorithm that converts encoded data back into its original format. In the context of communication systems, particularly those involving encoding for secure transmission, a decoder plays a crucial role in ensuring that the intended receiver can accurately recover the original message from the received codeword.\\n\\nIn secure communication, messages are often encoded using specific coding schemes to protect against eavesdropping and to ensure reliable transmission over noisy channels. The encoding process transforms the original message (often referred to as the plaintext) into a codeword, which is then transmitted over the communication channel. The decoder, located at the receiving end, is responsible for interpreting the received codeword and reconstructing the original message.\\n\\nKey functions of a decoder include:\\n\\n1. **Error Correction**: Decoders often incorporate error-correcting techniques to handle noise and interference that may corrupt the transmitted codeword. This ensures that the intended message can be recovered even if some bits are altered during transmission.\\n\\n2. **Mapping Codewords to Messages**: The decoder uses the specific coding scheme employed during encoding to map the received codeword back to the original message. This process may involve looking up a codebook or applying mathematical transformations.\\n\\n3. **Maintaining Security**: In the context of physical-layer security, decoders may also be designed to ensure that even if an eavesdropper intercepts the codeword, they cannot easily extract the original message. This is achieved by exploiting the randomness and noise in the communication channel.\\n\\nOverall, the effectiveness of a decoder is critical for the reliability and security of communication systems, allowing legitimate receivers to accurately interpret the information while preventing unauthorized access by eavesdroppers.'}]\n"
     ]
    }
   ],
   "source": [
    "user_profile = {}  # Dictionary to store user preferences\n",
    "\n",
    "def personalize_response(user_id, query, response):\n",
    "    \"\"\"Store responses for user history tracking\"\"\"\n",
    "    if user_id not in user_profile:\n",
    "        user_profile[user_id] = []\n",
    "    \n",
    "    user_profile[user_id].append({\"query\": query, \"response\": response})\n",
    "\n",
    "# Store response for a user\n",
    "personalize_response(\"user_123\", query, final_response)\n",
    "\n",
    "# Retrieve past user interactions\n",
    "print(\"User Interaction History:\", user_profile[\"user_123\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
